
1. Background

Hi, I'm Ben.

Julia is a Kaggle comp.

I'm using a smallish laptop for all of this (GET SPECS), so I'll be trying to keep networks small enough to keep training times reasonable, only a few minutes for each one.

The base for a lot of the starting code has been adapted (sometimes just outright copied) from the examples and excerices from the free TensorFlow course on Udemy (LINK). It's another great intro to using TensorFlow for neural networks, and I highly recommend giving it a go, either before, after or even alongside this tutorial.

2. What is a neural network?



3. Getting the data

 WHAT?

In this blog entry, we will be pre-processing the training images for our network to use, and storing them in a Pickle file.


 HOW?

Kaggle handily offers a 'Data' page for each of its competitions. For this particular competition, there are two sets of training data: one set of raw pictures, each with varying heights and widths, and one that they've resized to 20x20 pixels. As I'm keeping things small, the 20x20 images will do nicely for now. We can always play around with larger images once we've got a network we know works.

So, having gone here (LINK), downloaded the 'trainResized' .zip file and extracted it, I now have a folder full of 20x20 images to work with. Add to that the .csv file 'trainLabels' and we are ready to go.

The first thing I'll need to do is to get the images in a format that is relatively quick for my network to load every time. There are a couple of ways of doing this, but I'm going to choose to 'pickle' them. This essentially puts the data for all the images in a compressed file, keeping their original Python and Numpy structures.

We can break down the task into a few different parts. The first is to define a function which puts all of the images into a Numpy array, which the network will need in order to train itself.


	pickle_julia_blog.py - getImageData()


We then need to put the labels into a list so that we can match them to the images, and the network knows what the image is supposed to be. We do this by assigning each number and letter an ID, which will be integers ranging from 0 to 61. How you choose to assign the IDs is arbitrary, so long as each number and letter (upper and lower case) gets a unique ID between 0 and 61. I have chosen to assign numbers the IDs 0-9 (corresponding to their actual number), then the IDs 10-35 for upper case letters A-Z, and finally the IDs 36-61 for the lower case letters a-z.


	pickle_julia_blog.py - getLabels(), getLabelId()


All that remains to be done is to shuffle the dataset, select the size of the validation and test datasets, and pickle the final datasets and labels. We can then access the datasets, and corresponding labels, any time we want to, without having to load in and pre-process all of the images each time.


	pickle_julia_blog.py - shuffleDataAndLabels(), splitData(), saveData(), pickleFiles()


One final thought: check your work. In this instance, it is particularly easy to get one of the datasets you've just created, and check that its associated label is the correct one. My first major headache with this project came with my failure to check exactly this, and I was left scratching my head for a couple of days, unable to work out why my network subbornly chose to always just guess the most abundant training letter (an 'A' in my case), regardless of what image it was shown. It had simply worked out that since there was no correlation between the image and the label, it would get the highest score by guessing the most abundant training label, the clever little thing. Here is the code to check the labels match the images.


	pickle_julia_blog.py - showImages()


 WHY?

Why do we centre the pixel values around zero?

Generally speaking, neural networks work by creating an n-dimentional graph to represent a given probelm, and then try to find the global minima, which is the best possible solution to the problem the graph represents. Every time the network is given extra training data and tries to find this global minima, it works out the gradient of the graph around it in various dimentions, then 'goes down' the steepest one.
